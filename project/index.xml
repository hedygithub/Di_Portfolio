<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Academic</title>
    <link>https://hedygithub.github.io/Di_Portfolio/project/</link>
      <atom:link href="https://hedygithub.github.io/Di_Portfolio/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 30 Dec 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hedygithub.github.io/Di_Portfolio/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://hedygithub.github.io/Di_Portfolio/project/</link>
    </image>
    
    <item>
      <title>Question Answering on Long Context</title>
      <link>https://hedygithub.github.io/Di_Portfolio/project/nlp-qa/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://hedygithub.github.io/Di_Portfolio/project/nlp-qa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Portfolio Management Using Reinforcement Learning</title>
      <link>https://hedygithub.github.io/Di_Portfolio/project/rl-pm/</link>
      <pubDate>Wed, 08 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://hedygithub.github.io/Di_Portfolio/project/rl-pm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Anomaly Detection for Web Traffic Data</title>
      <link>https://hedygithub.github.io/Di_Portfolio/project/ts-ad/</link>
      <pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://hedygithub.github.io/Di_Portfolio/project/ts-ad/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Web traffic is the amount of data sent and received by visitors to a website. The web traffic data can be represented by a time series data to record activities on the website. Abnormal data points in this web traffic refers to abnormal changes of such traffic. Such abnormal change can be caused by network attacks. Thus, it is crucial to detect anomalies accurately and efficiently in the time series web traffic to further identity network attacks and prevent consequential economic and social losses.&lt;/p&gt;
&lt;p&gt;In this project, we experimented with both ARIMA and C-LSTM to perform anomaly detection on web traffic data, where ARIMA is a typical statistical approach and C-LSTM is an innovated deep learning structure applied on time series data. Experiments demonstrate that ARIMA performs differently on different types of anomalies as ARIMA focuses only on local data rather than a full picture. On the other hand, C-LSTM outperforms CNN alone, reaching a recall rate of 79.1%.&lt;/p&gt;
&lt;h2 id=&#34;data-overview&#34;&gt;Data Overview&lt;/h2&gt;
&lt;p&gt;The dataset that we use is extracted from the Yahoo Webscope program. The dataset consists of four benchmarks: A1Benchmark, A2Benchmark, A3Benchmark and A4Benchmark.  We choose to use A1Benchmark because it is based on real production of traffic data to some of the Yahoo web servers. The class A1 contains 67 files and each file has a different distribution of traffic. And there is exactly 94,866 datapoints in A1 file and 1669 of them are anomalies, which occupy 1.76%. Note that the timestamps of the A1Benchmark are replaced by integers with the increment of 1, where each datapoint represents one-hour worth of data. Even though an exact timestamp is not available, it is still possible to identify the daily and weekly seasonality given that each datapoint represents one-hour worth of data.&lt;/p&gt;
&lt;h2 id=&#34;report&#34;&gt;Report&lt;/h2&gt;
&lt;p&gt;Please review the report pdf for details.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Self/Semi-Supervised Image Classification</title>
      <link>https://hedygithub.github.io/Di_Portfolio/project/ssl-image-classfication/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      <guid>https://hedygithub.github.io/Di_Portfolio/project/ssl-image-classfication/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;At the end of Deep Learning course taught by Yann LeCun, a final project was assigned. We competed with your classmates on who can find the best self/semi-supervised learning algorithm. We were given a dataset with a large amount of unlabeled data and a small amount of labeled data to train the model, and the final performance of the model will be evaluated on a hidden test set and posted on a public leaderboard.&lt;/p&gt;
&lt;h2 id=&#34;data-overview&#34;&gt;Data Overview&lt;/h2&gt;
&lt;p&gt;The dataset, of color images of size 96×96, that has the following structure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;512, 000 unlabeled images.&lt;/li&gt;
&lt;li&gt;25, 600 labeled training images (32 examples, 800 classes).&lt;/li&gt;
&lt;li&gt;25, 600 labeled validation images (32 examples, 800 classes).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall, a hidden test set was kept, which will be used to test
the models. In order to improve performance when training our model with
few labeled samples, we need to make use of the unlabeled data.&lt;/p&gt;
&lt;h2 id=&#34;our-presentation&#34;&gt;Our presentation&lt;/h2&gt;
&lt;p&gt;Semi-supervised learning incorporates both labeled and unlabeled data points in the training process and aims to use leverage the unlabeled data to learn features that would support modeling process when using the training data for specific tasks.
The community has gained popularity as the amount of data required and cost of obtaining human labeled data increased over the years.
This video explains the modeling and thought process our team had when tackling the given task, and achieved 50% accuracy on the test set with CoMatch.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/NADnN1kq28w&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;all-presentations-at-deep-learning-spring-21-class&#34;&gt;All presentations at Deep Learning Spring 21 Class&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/MJfnamMFylo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Pyspark Song Recommender System</title>
      <link>https://hedygithub.github.io/Di_Portfolio/project/recommender-system/</link>
      <pubDate>Fri, 30 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://hedygithub.github.io/Di_Portfolio/project/recommender-system/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this final project, we apply big data tools to build and evaluate a collaborative filter based recommender system. The dataset we
work on is the Million Song Dataset (MSD), with implicit user feedback. Using Spark’s alternating least squares (ALS) method, we
learn latent factor representations for users and items, and recommend for users in the test set. Thereafter, we compare our model to
single-machine implementation, and the baseline for the extension.&lt;/p&gt;
&lt;h2 id=&#34;data-overview&#34;&gt;Data Overview&lt;/h2&gt;
&lt;p&gt;Data used for the basic recommendation system consists of the train, validation, and test parquet files. Each row in the files consists of
user_id (string), count (int), and track_id (string). There are 49,824,519 records for the train, 135,938 records for the validation, and
1,368,430 for the test. Additional data including metadata, features, genre tags, lyric, are also used for the extension&lt;/p&gt;
&lt;h2 id=&#34;report&#34;&gt;Report&lt;/h2&gt;
&lt;p&gt;Please review the report pdf for details.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Aspect-Based Sentiment Analysis</title>
      <link>https://hedygithub.github.io/Di_Portfolio/project/nlp-absa/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://hedygithub.github.io/Di_Portfolio/project/nlp-absa/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Sentiment Analysis (SA) aims at classifying the sentiment polarity towards a whole sentence.
Compared to SA, Aspect-Based Sentiment Analysis (ABSA) is designed to identify certain target aspects of an entity and classify sentiment polarities towards these aspects.
For example, in the sentence “Food is pretty good but the service is horrific”, there are two target aspects: “food” and “service” with opposite sentiment polarities.
Further, there are two variants of the ABSA problem. One is Aspect-Target Sentiment Classification (ATSC), which is our example before and also the focus of our paper.&lt;/p&gt;
&lt;p&gt;In recent years, neutral networks have been developed and largely improved the ABSA performance by learning target-context relationships.
Afterwards, the pre-trained language model shows powerful representation ability.
Its application to many down-stream tasks, including ABSA, has achieved many accomplishments.
Lately, domainspecific post-trained BERT shows better performance on this topic. In this paper, we experiment with LSTM-based and BERT-based models for aspect-based sentiment analysis, and apply these models to a more challenging dataset than the commonly used benchmark. We then conduct a robust error analysis for our models to analyze reasons for erroneous classifications.&lt;/p&gt;
&lt;h2 id=&#34;our-presentation&#34;&gt;Our presentation&lt;/h2&gt;
&lt;p&gt;We implement LSTM-based models (LSTM and ATAE-LSTM) and BERT-based models (vanilla BERT-base and BERT-ADA) on a challenging dataset called MAMS, which contains multiple aspects with multiple sentiment polarities.
We also conduct error analysis through the method of input reduction. The experiment results show that BERT-ADA outperforms other models on MAMS dataset. You can find more details in the video.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/jacbPazwgZ4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Predict number of views for videos on Bilibili</title>
      <link>https://hedygithub.github.io/Di_Portfolio/project/bilibili/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://hedygithub.github.io/Di_Portfolio/project/bilibili/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bear Maps</title>
      <link>https://hedygithub.github.io/Di_Portfolio/project/map/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://hedygithub.github.io/Di_Portfolio/project/map/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This project was originally created by former TA Alan Yao (now at AirBNB). It is a web mapping application inspired by Alan’s time on the Google Maps team and the OpenStreetMap project, from which the tile images and map feature data was downloaded.&lt;/p&gt;
&lt;p&gt;In this project, you’ll build the “smart” pieces of a web-browser based Google Maps clone. This is typical in real world programming, where you don’t have the luxury and freedom that comes with starting from totally blank files. As a result, you’ll need to spend some time getting to know the provided code, so that you can complete the parts that need completing. The map should have these functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Zoom in and zoom out the map.&lt;/li&gt;
&lt;li&gt;Autocomplete the search input. Prefix is the partial query string. The prefix will be a cleaned name for search that is: (1) everything except characters A through Z and spaces removed, and (2) everything is lowercased. The method will return a list containing the full names of all locations whose cleaned names share the cleaned query string prefix, without duplicates.&lt;/li&gt;
&lt;li&gt;Search. The user should also be able to search for places of interest. Implementing this method correctly will allow the web application to draw red dot markers on each of the matching locations.&lt;/li&gt;
&lt;li&gt;Turn-by-turn navigation. As an extra-challanging feature, you can use your A* search route to generate a sequence of navigation instructions that the server will then be able to display when you create a route.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By the end of this project, with some extra work, you can even host your application as a publicly available web app. More details will be provided at the end of this spec.&lt;/p&gt;
&lt;p&gt;More details about the project can be found: &lt;a href=&#34;https://sp19.datastructur.es/materials/proj/proj2c/proj2c#introduction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sp19.datastructur.es/materials/proj/proj2c/proj2c#introduction&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;my-publicly-available-web-app-on-heroku&#34;&gt;My Publicly Available Web App on Heroku&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://bearmaps-cs61b-sp19-hedy.herokuapp.com/map.html#lat=37.871826&amp;amp;lon=-122.260086&amp;amp;depth=3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://bearmaps-cs61b-sp19-hedy.herokuapp.com/map.html#lat=37.871826&amp;lon=-122.260086&amp;depth=3&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Suggested Store Locations</title>
      <link>https://hedygithub.github.io/Di_Portfolio/project/store-location/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://hedygithub.github.io/Di_Portfolio/project/store-location/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;With the fast development of China consumer market, a global fast-fashion brand plans to enter China market. And they have chosen Shanghai, the most fashion city in China, as their first entered city. At first stage, the company plans to open about 8~10 stores, based on their judgments of the market and company’s financial ability.&lt;/li&gt;
&lt;li&gt;The business problem is where are the best locations to open those stores. Without considering rent costs, the company should open a store in the most popular place, where bring stores high traffic and target customers. Therefore, we must know where the company’s target customer mostly like to visit. It can be breakdown to two questions. First, how to find the place our target customers like to visit. In this case, our customer is those 18~30 girls who like fashion clothes, shoes, and bags. Second, where is the most popular places among all the places they like.&lt;/li&gt;
&lt;li&gt;Another problem is to decide the store opening sequence and the possible shopping malls near those optimal locations to open those stores. Maybe we can rank those popular places according to their popularity and open store according to this rank. And we can find nearby shopping malls as a list to give the company’s mangers as a reference.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;report&#34;&gt;Report&lt;/h2&gt;
&lt;p&gt;Suggested new store locations using K-means and DBSCAN clustering with parsed geolocation data including public transport locations, E-commerce delivery addresses, competitors&#39; store locations. More details in the pdf and slides.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
