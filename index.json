[{"authors":null,"categories":null,"content":"Di is a second year master student from New York University majored in Data Science. She is passionate about applying machine learning, Deep Learning and NLP to real world business.\nShe have 3-years data related working experiences in Consulting, Finance and Retail industry. Her projects covered Regression, Classfication, Clustering, Deep Learning, NLP and recommendation system. Her research interests include Self/Semi-Supervised Learning and Text Analysis.\n  Download my resum√©.\n","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1567641600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://hedygithub.github.io/Di_Portfolio/author/di-he/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/Di_Portfolio/author/di-he/","section":"authors","summary":"Di is a second year master student from New York University majored in Data Science. She is passionate about applying machine learning, Deep Learning and NLP to real world business.","tags":null,"title":"Di He","type":"authors"},{"authors":null,"categories":null,"content":"  -- Table of Contents  Topics in this note    Models in Machine Learning - Machine Learning Process -- Topics in this note  Introduction to ML Models Include: Linear Regression, LASSO and Ridge, Logistic Regression, Naive Bayes, Support Vector Machine, Decision Tree, Ensemble Overview \u0026amp; Stacking, Bagging \u0026amp; Random Forest, Boosting, AdaBoost \u0026amp; Gradient Boosting, K-Nearest Neighbors, Clustering Overview \u0026amp; K-Means   ML General Knowledge Introduction to Machine Learning General knowledge   Di He ## FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   -- ","date":1629763200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629763200,"objectID":"ba742626ac63643fb3a92981ae3514d0","permalink":"https://hedygithub.github.io/Di_Portfolio/notes/machine-learning/","publishdate":"2021-08-24T00:00:00Z","relpermalink":"/Di_Portfolio/notes/machine-learning/","section":"notes","summary":"Introduction to machine learning basic models, like Regression, Classification, Trees, Clustering. Also introduce some basic concepts and workflows in Data Scicence.","tags":null,"title":"Machine Learning","type":"book"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Di He FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://hedygithub.github.io/Di_Portfolio/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/Di_Portfolio/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Include: Linear Regression, LASSO and Ridge, Logistic Regression, Naive Bayes, Support Vector Machine, Decision Tree, Ensemble Overview \u0026amp; Stacking, Bagging \u0026amp; Random Forest, Boosting, AdaBoost \u0026amp; Gradient Boosting, K-Nearest Neighbors, Clustering Overview \u0026amp; K-Means\n  2 hours to read\n  -- Main References  Introduction to Data Science (NYU CDS 1001) Book: Data Science for Business Notes of Probability and Statistics for Data Science (NYU CDS 1002) Optimization and Computational Linear Algebra for Data Science (NYU CDS 1002) Bruce Yang: The Breadth of Machine Learning: Part I K-Means Clustering in Python: A Practical Guide  Linear Regression  Basic Concepts       Xs are the covariates (or features, or inputs, or independent variables) Y is the response (or outcomes, or outputs, or dependent variable. Noise Term (or errors): i.i.d. Gaussian random variables Residuals: The errors in our predictions  Assumptions  Linearity:  There is a linear relationship between the covariates and the response. Linear relationship can be assessed with scatter plots.   Normality:  For any fixed value of X, Y is normally distributed. Normality can be assessed with histograms. Normality can also be statistically tested, for example with the Kolmogorov-Smirnov test. When the variable is not normally distributed a non-linear transformation like Log-transformation may fix this issue.   The Noise Term  The error term is assumed to be a random variable that has a mean of 0 and normally distributed (i.i.d. Gaussian random variables). (Residuals are statistically independent, have uniform variance, are normally distributed, have no auto-correlation) Based on this assumption, we can construct confidence intervals and perform hypothesis tests about our parameters.    The normally distributed assumption can be relaxed if we have enougth data.   Homoscedasticity  The error term has onstant variance œÉ2 at every value of X. Why we need homoscedasticity?    There are tests and plots to determine homescedasticity. Residual plots, White‚Äôs test and Breusch-Pagan., Levene\u0026rsquo;s test, Barlett\u0026rsquo;s test, and Goldfeld-Quandt Test. If it is heteroscedastic, we can use Weighted Least Squares (WLS) to transform the problem into the homoscedastic case. If we cannot perform weighted least squares or a variable transformation and must use ordinary least squares, the confidence intervals and hypothesis tests are no longer valid. There are techniques such as Huber-White that attempt to address these issues.   Non-Collinearity (Xs is full column rank in Linear Algerba):  Multicolinearity occurs when the independent variables are correlated with each other. Why : Multicolinearity means Xs has no full column rank, and by rank-nullity theorem, dimension of ker(Xs) bigger than 0. Hence, it will have more than one soulution. This can be check by heat map of correlation.    Solutions: Mean Squared Error (MSE)  Simple Linear Model:    General Linear Model (Squared Error is a Convex Funcion):    Unbiased Estimation (MSE):    Hypothesis Tesing \u0026amp; Confidence Interval: for Œ≤0, Œ≤1, for Œ≤0 + Œ≤1x (fitted Y) Predicted Interval: Note that prediction intervals are slightly diÔ¨Äerent from conÔ¨Ådence intervals, since Y is random (along with the endpoints of the interval). Our prediction interval for Y will incorporate our uncertainty in estimating Œ≤0 + Œ≤1x, and the noise Z present in Y.  Useful Points of Linear Model  In simple linear model, the fitted line pass through the sample central point. In simple linear model, if X and Y are standardized, correlation is equal to slope. Centerated Resduals. In linear model, sum of residual equal to zero. Reduced Variance:  R-squared: proportion of variance explained by the Ô¨Åt Adjusted R-squared: compared with R-squared, it isn‚Äôt guaranteed to grow as we add features (due to the n‚àík denominator that penalizes larger models), and thus can be more useful. Other methods for weighing goodness-of-Ô¨Åt against model complexity include the AIC, BIC, and Mallows‚Äôs Cp.   Leverage: Slope has the highest sensitivity to points furthest from the mean  Advantages/Disadvantages Pros:\n Simplicity and interpretability: linear regression is an extremely simple method. It is very easy to use, understand, and explain. The best fit line is the line with minimum error from all the points, it has high efficiency It needs little tuning  Cons:\n Linear regression only models relationships between dependent and independent variables that are linear. It assumes there is a straight-line relationship between them which is incorrect sometimes. Linear regression is very sensitive to the outliers in the data (See Leverage). Linear regression is very sensitive to missing data. (biased parameter) Linear regression needs feature scaling. (for gradient descent) If the number of the parameters are greater than the samples, then the model starts to model noise rather than relationship Correlated features may affect performance. Extensive feature engineering required.  Quesion Part Question 1.1: Missing feature If you Ô¨Åt a linear model that has some features missing, will your least squares estimates of the reduced model be biased?\n It will be Biased, unless the omitted features are uncorrelated with the included features.  Question 1.2: Extra feature If you Ô¨Åt a linear model that has some extra features, will your least squares estimates of the enlarged model be biased?\n It will be Unbiased. Even though adding features does not introduce bias (and can decrease it), it can increase the variance of our estimates and produce larger conÔ¨Ådence intervals and prediction intervals.  Question 2: More Data What if you duplicate all the data and do regression on the new data set?\n The mean and variance of the sample would not change therefore the beta estimation would be the same. The standard error will go down. However, since the sample size is doubled this will result in the lower p-value for the beta. This tells us that by simply doubling/duplicating the data, we could trick the regression model to have smaller confidence interval.  LASSO and Ridge  Basic Concepts      Comparision For identical features (High Corr. \u0026amp; Same Scale):\n L1 regularization spreads weight arbitrarily (all weights same sign) L2 \u0026amp; Elastic Net regularization spreads weight evenly  For linearly related features (High Corr. \u0026amp; Diff Scale):\n L1 regularization chooses variable with larger scale, 0 weight to others L2 \u0026amp; Elastic Net regularization prefers variables with larger scale ‚Äì spreads weight proportional to scale  For correlated features (Corr. \u0026amp; Same Scale):\n L1 regularization: Minor perturbations (in data) can drastically change intersection point (very unstable solution). Makes division of weight among highly correlated features (of same scale) seem arbitrary. L2 \u0026amp; Elastic Net regularization: correlated random variables (similar scale) have similar coefficients.  Advantages/Disadvantages of LASSO Pros:\n Useful for feature selection Much easier to interpret and produces simple models Lasso will perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero  Cons:\n LASSO has no closed formula LASSO needs feature scaling. (for fair regularization to parameters) For n \u0026lt; p case (high dimensional case), LASSO can at most select n features. This has to do with the nature of convex optimization problem LASSO tries to minimize. For usual case where we have correlated features which is usually the case for real word datasets, LASSO will select only one feature from a group of correlated features. That selection also happens to be arbitrary in nature. Often one might not want this behavior. Like in gene expression the ideal gene selection method is: eliminate the trivial genes and automatically include whole groups into the model once one gene among them is selected (‚Äògrouped selection‚Äô). LASSO doesn\u0026rsquo;t help in grouped selection.  Advantages/Disadvantages of Ridge Pros:\n Useful for preventing overfitting. As lambda increases, the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions, at the expense of a slight increase in bias. Ridge regression works best in situations where the least squares estimates have high variance. Meaning that a small change in the training data can cause a large change in the least squares coefficient estimates Ridge regression has unique solution and has substantial computational advantages.  Cons:\n Ridge regression is not able to shrink coefficients to exactly zero. As a result, it cannot perform feature selection. LASSO needs feature scaling. (for fair regularization to parameters)  Quesion Part Question 1: Feature Selection Why LASSO has the property of feature selection but Ridge does not?\n By Intuition      By Lasso Solution: (Note there is not closed form formula for Lasso, unless A has orthnormal matrix)     Question 2: Regularization What is regularization?\n What: Regularization is used to prevent overfitting. It significantly reduces the variance of the model, without substantial increase in its bias. It will improve the generalization of a model and decrease the complexity of a model. How: It adds a penalty on the loss function to reduce the freedom of the model. Hence the model will be less likely to fit the noise of the training data.  Question 3: Parameter Tuning How to choose Lambda?\n Lambda is the tuning parameter that decides how much we want to penalize the flexibility of our model. As lambda increases, the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero. Selecting a good value of lambda is critical, we can use cross validation to choose good lambda.  Logistic Regression  Basic Concept: Logistic Regression is a classification method, usually do binary classification 0 or 1. A logistic model is one where the log-odds(logit) of the probability of an event is a linear combination of independent variables.   Assumptions:  The outcome is a binary variable like yes vs no, positive vs negative, 1 vs 0. There is a linear relationship between the logit of the target and independent variables. Others similiar to linear regression, such as multi-collinearity.  Cost function:   Log-loss (Cross-Entropy)     Understand Cross-Entropy from information theory:     MLE: For large data, the theory of MLEs can be used to show that the parameter estimates are jointly normally distributed, and conÔ¨Ådence intervals can be computed.\n  The difference between the cost function and the loss function: The loss function computes the error for a single training example; the cost function is the average of the loss function of the entire training set.\n  Advantages/Disadvantages Pros:\n Outputs have a nice probabilistic interpretation. Based on MLE, we can do hypothesis testing on the parameter estimates of the model. The algorithm can be regularized to avoid overfitting. Multi-collinearity is not really an issue and can be countered with L2 regularization to an extent. Logistic Regression has been proven over and over to be very robust in small data problems, because it has strong assumption. For example: Learning curve analysis shows that LR performs better than DT in small data scenarios) Logistic models can be updated easily with new data using stochastic gradient descent. Linear combination of parameters Œ≤ and the input vector will be incredibly easy to compute. Wide spread industry comfort for logistic regression solutions.  Cons:\n Logistic regression tends to underperform when there are multiple or non-linear decision boundaries. They are not flexible enough to naturally capture more complex relationships. Doesn‚Äôt handle large number of categorical features/variables well.  Extension: Another Perspective to Regularization - Bayesian MAP    Extension: Activation Functions (in Neural Network) Ref. 7 types neural network activation functions   Sigmoid / Logistic  Pros  Smooth gradient, preventing ‚Äújumps‚Äù in output values. Output values bound between 0 and 1, normalizing the output of each neuron. Clear predictions: for X above 2 or below -2, tends to bring the Y value (the prediction) to the edge of the curve, very close to 1 or 0. This enables clear predictions.   Cons  Vanishing gradient: for very high or very low values of X, there is almost no change to the prediction, causing a vanishing gradient problem. This can result in the network refusing to learn further, or being too slow to reach an accurate prediction. Outputs not zero centered. Computationally expensive    TanH / Hyperbolic Tangent  Pros  Zero centered‚Äîmaking it easier to model inputs that have strongly negative, neutral, and strongly positive values. Otherwise like the Sigmoid function.   Cons  Like the Sigmoid function    ReLU (Rectified Linear Unit)  Pros  Computationally efficient‚Äîallows the network to converge very quickly Non-linear: although it looks like a linear function, ReLU has a derivative function and allows for backpropagation   Cons  The Dying ReLU problem: when inputs approach zero, or are negative, the gradient of the function becomes zero, the network cannot perform backpropagation and cannot learn.    Extension: Softmax (Muti-class Activation Function)     Pros  Able to handle multiple classes: only one class in other activation functions‚Äînormalizes the outputs for each class between 0 and 1, and divides by their sum, giving the probability of the input value being in a specific class. Useful for output neurons‚Äîtypically: Softmax is used only for the output layer, for neural networks that need to classify inputs into multiple categories.   Usage in torch:  NLLLoss: The negative log likelihood loss. It is useful to train a classification problem with C classes. The input given through a forward call is expected to contain log-probabilities of each class. Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. CrossEntropyLoss: If you prefer not to add an extra layer (LogSoftmax), you may use CrossEntropyLoss instead. CrossEntropyLoss combines nn.LogSoftmax() and nn.NLLLoss() in one single class.    Niave Bayes  Basic Concepts \u0026amp; Assumptions Naive Bayes is a generative model, and is used for classification problems, especially text classification. Many language processing tasks can be viewed as tasks of classiÔ¨Åcation. Text categorization,in which an entire text is assigned a class from a Ô¨Ånite set, includes such tasks as sentiment analysis, spam detection, language identiÔ¨Åcation, and authorship attribution. Sentiment analysis classiÔ¨Åes a text as reÔ¨Çecting the positive or negative orientation (sentiment) that a writer expresses toward some object.\n Bayes' Rule:    Assumptions:  the bag of words assumption (position doesn‚Äôt matter) the conditional independence assumption (words are conditionally independent of each other given the class; the occurrence of a certain feature is independent of the occurrence of other features)       Laplace Smoothing (Variance-Bias Trade-off) If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as ‚ÄúZero Frequency‚Äù. To keep a model from assigning zero probability to these unseen events, we‚Äôll have to shave off a bit of probability mass from some more frequent events and give it to the events we‚Äôve never seen. This modiÔ¨Åcation is called smoothing or discounting. Those method will decrease variance at the cost of increasing bias. The simplest way to do smoothing is to add one to all the counts, which is Laplace Smotthing.   Pseudo Code of Naive Bayes with Laplace Smoothing    Advantages/Disadvantages Pros:\n It is easy and fast to predict class of test data set. Needs less training time. Good with moderate to large training data sets. It could be used for making predictions in real time. It also perform well in multi class prediction. Naive Bayes Classifier and Collaborative Filtering together could build a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not Good when dataset contains many features. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.  Cons:\n On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously. Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.  Extension: Conjugate Prior   In Bayesian probability theory, if the posterior distributions p(Œ∏ | x) are in the same probability distribution family as the prior probability distribution p(Œ∏), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function p(x | Œ∏).     Examples\n Beta prior, Bernoulli samples Normal prior, Normal samples Gamma prior, Poisson samples    Understanding Laplace Smoothing   The beta distribution is the conjugate prior of the Binomial distribution. It is a special form of the Dirichlet distribution, where X has only two discrete values. The Dirichlet prior has a specific application to Na√Øve Bayes because X is often defined as a multinomial. In many cases we only want to deal with a binary random variable, which makes the beta distribution appropriate.\n  Support Vector Machine  Ref. Wikipedia: Support Vector Machine\nBasic Concepts  The goal of SVM is to design a hyperplane that classifies all training vectors in 2 classes. The best choice will be the hyperplane that leaves the maximum margin from both classes. (Hyperplane: is a linear decision surface that splits the space into 2 parts, it is obvious that a hyperplane is a binary classifier. A hyperplane is a p-1 dimension: in two dimensions, a hyperplane is a line, in three dimensions, a hyperplane is a plane.) As a rule of thumb, SVMs are great for relatively small data sets with fewer outliers.     Hard Margin and Soft Margin (hinge-loss)      Solving the Problem through Dual Problem and Quadratic Programing    Kernel Trick    Advantages/Disadvantages Pros:\n Strong geometric interpretation Easily extendable to non-linear decision surfaces, with kernel trick Only use support vextor, the risk of overfitting is less in SVM.  Cons:\n Choose a \u0026ldquo;good\u0026rdquo; kernel function is not easy. Can not give probability output. Difficult to understand and interpret the final model, varaible weights and individual impact.  Extension: The Representaion Theory and Kernelization        Extension: Mutli-Class        Decision Tree   Decision tree uses a tree structure to specify sequences of decisions and consequences. It breaks down a dataset into smaller subsets based on the important features. There are Regression Tree and Classification Tree. A decision tree employs a structure of nodes and branches: Root node, Internal Node, and Leaf Node. Tree-based methods tend to perform well on unprocessed data (i.e. without normalizing, centering, scaling features).  Finding and selecting informative attributes Entopy and Information Gain Character: Prefer feature(C) with more unique values(c)\n Entropy: the average amount of information that is encoded by a random variable X. Conditional Entropy: Given we know X, how much extra information is needed to encode Y. Information Gain (IG): How much new information do we gain on Y by conditioning on X.     Gain Ration  Gain Ration = IG(C) / IV(C) IV intrinsic value = count unique(c of C)  Combined  First, IG \u0026gt; Constant A Then, Rank by Gain Ratio  Pseudocode    Control Complexity Hyperparams Tuning  Depth of Tree (+) Min Leaf Size (-) Min Split Size (-)     Pruning  Pre-Pruning Post-Pruning  Advanced Parts Feature Exploration Often times the Decision Tree is useful as tool for testing for feature interactions as well as ranking features by their ability to predict the target variable. Scikit-learn‚Äôs DecisionTree fit function automatically returns normalized information gain for each feature (also called the Gini Importance).\nNumeric Variables  Features: Use Split Points to discretize it. Try all split points, and choose the highest IG. Target Variable(Regression): Measure Purity  Variance Weighted Average Variance Reduction    Probability Estimation  Frequency-Based Estimation (FBE): If we are satisfied to assign the same class probability to every member of the segment corresponding to a tree leaf, we can use instance counts at each leaf to compute a class probability estimate. p(c) = Count(c) / (Count(all) Frequency-Based Estimation with Laplace correction: p(c) = (Count(c) + k) / (Count(all) + Unique(c) * k). It is useful in small samples. As number of instance increases: converge to the FBE.  Advantages/Disadvantages Pros:\n Easy to interpret (though cumbersome to visualize if large). Easy to implement just a series of if-¬≠then rules. Prediction is cheap: total operations = depth of tree. No feature engineering necessary. Can often handle categorical/text features as is (software dependent). Automatically detect non-¬≠linearities and interactions.  Cons:\n Easy to overfit: flexibility of algorithm requires careful tuning of parameters and leaf pruning. Decision Tree algorithms are greedy: not very stable and small changes in daat can give very different solutions. Difficulty learning in skewed target distributions (entropy is very small at the begining). Not well suited for problems as number of samples shrinks while number of features grows. Decision Tree is often relatively inaccurate when dataset is samll (less assumptions). Many other predictors perform better with similar data.  Question Part Question 1: Logistic Regression versus Tree Induction Classification trees and linear classifiers both use linear decision boundaries, what are the differences between them:\n Decision Boundary Diresctio: A classification tree uses decision boundaries that are perpendicular to the instancespace axes, whereas the linear classifier can use decision boundaries of any direction or orientation. This is a direct consequence of the fact that classification trees select a single attribute at a time whereas linear classifiers use a weighted combination of all attributes. Subspace Counts:  A classification tree is a ‚Äúpiecewise‚Äù classifier that segments the instance space recursively when it has to, using a divide-and-conquer approach. In principle, a classification tree can cut up the instance space arbitrarily finely into very small regions. A linear classifier places a single decision surface through the entire space. It has great freedom in the orientation of the surface, but it is limited to a single division into two segments. This is a direct consequence of there being a single (linear) equation that uses all of the variables, and must fit the entire data space.    Ensemble Overview \u0026amp; Stacking  Ensemble Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).\nTyptes of Ensemble  Stacking  Stacking is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. Often considers heterogeneous base learners. Example: Taking a weighted combination of the predictions of a total of S different classifiers.   Bagging  Decrease variance The base learners are generated in parallel through resampling. Often considers homogeneous base learners. The basic motivation of parallel methods is to exploit independence between the base learners since the error can be reduced dramatically by averaging.   Boosting  Decrease bias The base learners are generated sequentially, using cumulative errors to inform or weight additional classifiers. Often considers homogeneous base learners. The basic motivation of sequential methods is to exploit the dependence between the base learners. The overall performance can be boosted by weighing previously mislabeled examples with higher weight.       Stacking Pseudocode    Stacking Training Data In order to generalize appropriately, the meta-model needs to be built from data that is seperate from the base-models. Otherwise, the system risks overfitting. Note that this is best accomplished when sample sizes are large.\nQuesion Part Question 1: Why stacking work?  Case 1: In the case of simple averaging, the weighted average model de-noises individual models that are poentially overfir around the decision boundaries. Case 2: Different models perform better on different parts of the input space S. For example: more complex model will do better on regions of X with higher support(more samples); more biased models may then be better where there is lower support (less samples). Then, Stacking learns the best of both worlds.  Bagging \u0026amp; Random Forest  Bootstraping (statistics) ‚ÄúBootstrapping is a statistical procedure that resamples a single dataset to create many simulated samples. This process allows for the calculation of standard errors, confidence intervals, and hypothesis testing‚Äù (Forst)   Bagging Bootstrap aggregation, or bagging is a general-purpose procedure for reducing the variance of a statistical learning method.   RandomForests Basic Concept The Random Forest algorithm is probably the most well known and utilized implementation of the Bagging technique. A Random Forest is an ensemble of Decision Trees, where both bagging and random feature selection are used to add randomness and reduce the variance of the forest.   Why do Random Forests work  Bias  A single decision tree is unstable, and has high variance, but can also have extremely low bias. It can detact a all manner of interaction effects, specially when allowed to grow very deep The bias of the average of identically distributed trees is equal to the bias of the individual trees (in this case, very low).   Variance     Hyperparameters Tuning: Usually over-fit the individual trees, and use some hold-out method to optimize forest level parameters.\n Tree Level Paramters  max_depth: the size of the tree. Usually you don‚Äôt want to limit this min_sample_split: the number of instances in an intermediate node, before splitting (usually good to set to 1)   Forest Level Parameters  n_estimators: the number of trees (and bootstrapped samples) to be used max_features: the number of features that will be randomly sampled for each tree. The default in RandomForestClassifier is max_features=sqrt(total_features), which is generally a good suggestion. The default for n_estimators is 10, which is probably too low.    Feature Importance Much like with Decision Trees, the Random Forest Classifier has a built in mechanism for evaluating feature importance. Quoting the sklearn documentation: Features used at the top of the tree contribute to the final prediction of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features. The above computation is made for each feature in each tree and then averaged over all trees in the forest. The Random Forest Classifier returns an attribute with an importance score for each feature, and these scores sum to 1 across all features.\nOut-of-Bag Error  Cross-validation with Random Forest\u0026rsquo;s can be painfully slow. That\u0026rsquo;s because each cross-validation step requires building k * n_estimators trees. \u0026ldquo;Out-of-bag\u0026rdquo; error calculation. Each tree is built from a bootstrap sample of the data, so that for each tree, some portion of the data is not used for that tree. The Random Forest method computes an out-of-bag prediction for each record [ùë•ùëñ,ùë¶ùëñ] by averaging the prediction ùëìùëè(ùë•ùëñ,ùë¶ùëñ) on record ùëñ for the bootstrap iterations in which record ùëñ was not chosen in the bootstrap. The out-of-bag prediction can then be used to compute out-of-sample error for model selection and validation. This method should be equivalent to ùëÅ-fold cross-validation.  Advantages/Disadvantages Pros:\n A single decision tree tends to overfit the data. The process of averaging or combining the results of different decision trees helps to overcome the problem of overfitting. Random forests are extremely flexible and have very high accuracy. They also do not require preparation of the input data. You do not have to scale the data. It also maintains accuracy even when a large proportion of the data are missing.  Cons:\n The main disadvantage of Random forests is their complexity. They are much harder and time-consuming to construct than decision trees. They are less intuitive. When you have a large collection of decision trees it is hard to have an intuitive grasp of the relationship existing in the input data. In addition, the prediction process using random forests is time-consuming than other algorithms.  Question Part Question 1: Bootstraping versus Traditional Statistical Method Please compare Bootstraping versus Traditional Statistical Method, and state why we use former?\n Results derived from bootstraping are basically identical to those of the traditional approach. Both rely largely on the observed data. If the observed data contains outliers, both may skew the estimates. The traditional procedure requires one to have a test statistic that satisfies particular assumptions in order to achieve valid results, and this is largely dependent on the experimental design. The traditional approach also uses theory to tell what the sampling distribution should look like, but the results fall apart if the assumptions of the theory are not met. The bootstrapping method, on the other hand, takes the original sample data and then resamples it to create many [simulated] samples. This approach does not rely on the theory since the sampling distribution can simply be observed, and one does not have to worry about any assumptions. This technique allows for accurate estimates of statistics, which is crucial when using data to make decisions.  Boosting, AdaBoost \u0026amp; Gradient Boosting  Boosting Boosting works in a similar way with bagging, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is Ô¨Åt on a modiÔ¨Åed version of the original data set.     AdaBoost Adaboost, shortened for Adaptive Boosting, is an machine learning approach.The idea is to set weights to both classifiers and data points (samples) in a way that forces classifiers to concentrate on observations that are difficult to correctly classify. This process is done sequentially in that the two weights are adjusted at each step as iterations of the algorithm proceed. This is why Adaboost is referred to as a sequential ensemble method: ensemble referring to a type of learning that combines several models to improve the final predictive performance.   Mathmatical Process in Watermelon Book P173~177 Loss Function  Why Not 0-1 loss: The 0-1 loss function has nice properties that we would like to take advantage of for many problems. However, because it is not convex, it is difficult to optimize using the 0-1 loss, so we often turn to convex surrogate loss functions. What Surrogate Loss Function AdaBoost Use: Exponential Loss  Regularization  which weak classifier might work best to solve their given classification problem The number of boosting rounds that should be used during the training phase.  Advantages/Disadvantages Pros:\n AdaBoost is a powerful classification algorithm that has enjoyed practical success with applications in a wide variety of fields, such as biology, computer vision, and speech processing. Unlike other powerful classifiers, such as SVM, AdaBoost can achieve similar classification results with much less tweaking of parameters or settings (unless of course you choose to use SVM with AdaBoost). It enables a user to add several weak classifiers to the family of weak classifiers that should be used at each round of boosting. The AdaBoost algorithm will select the weak classifier that works best at that round of boosting. Cons: AdaBoost can be sensitive to noisy data and outliers. In some problems, however, it can be less susceptible to the overfitting problem than most learning algorithms.  Gradient Boosting Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\nLoss Function It depends on the model, could be square loss or exponential loss. For any loss function, we can derive a gradient boosting algorithm. Absolute loss and Huber loss are more robust to outliers than square loss.\nInroduction of \u0026ldquo;gradient\u0026rdquo;      Pseudocode    Regularization  Gradient boosting iterations M (i.e. the number of trees in the model when the base learner is a decision tree). Increasing M reduces the error on training set, but setting it too high may lead to overfitting. An optimal value of M is often selected by monitoring prediction error on a separate validation data set. Besides controlling M, several other regularization techniques are used. Depth of the trees. The higher this value the more likely the model will overfit the training data. Shrinkage     Gradient Boosting Decision Trees (GBDT) XGBoost XGBoost from the university of washington and published in 2016 introduces two techniques to improve performance.\n Firstly the idea of Weighted Quantile Sketch, which is an approximation algorithm for determining how to make splits in a decision tree (candidate splits). The second is Sparsity-aware split finding which works on sparse data, data with many missing cells.  LightGBM LightGBM from Microsoft and published in 2017 also introduces two techniques to improve performance.\n Gradient-based One-Side Sampling which inspects the most informative samples while skipping the less informative samples. And Exclusive Feature Bundling which takes advantage of sparse datasets by grouping features in a near lossless way.  Advantages/Disadvantages Pros:\n Often provides predictive accuracy that cannot be beat. If you are able to use correct tuning parameters, they generally give somewhat better results than Random Forests. Lots of flexibility: can optimize on different loss functions and provides several hyperparameter tunning options that make the function fit very flexible. No data preprocessing required, often works great with categorical and numerical values as is. Handles missing data, imputation not required.  Cons:\n GBMs will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting. Must use cross-validation to neuralize. Computationally expensive, GBMs often require many trees (\u0026gt;1000) which can be time and memory exhaustive. The high flexibility results in many parameters that interact and influence heavily the behavior of the approach. This requires a large grid search during tunning. Less interpretable although this is easily addressed with various tools (varaible importance, partial dependence plots, SHAP, LIME, etc.)  K-Nearest Neighbors  Basic Concept    Characters  Non-¬≠parametric You don\u0026rsquo;t have to make any assumptions about the functional form that estimates E[X|X]. This makes it very powerful for estimating any arbitary decision curve, but extreme flexibility always risks overfitting. Instance-¬≠based learning You technically don\u0026rsquo;t need to train it. Estimation of E[Y|X] is done locally and a scoring time by taking the average of Y of the k nearest neighbors of the instance. There is effectively no model, just all of the training data stored in memory.  Parameters Number of Neighbors The less, the more likely to over-fitting\nDistance Properties (similiar to norm) Given two points a and b, and a distance function d():\n d(a,b) ‚â• 0 ‚Ä¶ non-¬≠negativity d(a,b) = 0 only if and only if a=b d(a,b) = (b,a) ‚Ä¶ symmetry d(a,c) ‚â§ d(a,b)+d(b,c) ‚Ä¶ triangle inequality  Types Lp-¬≠norm distance measures\n Manhattan Distance Euclidean Distance  Scale Matters Features with higher magnitude tend to dominate distance metrics. It may be required to normalize the data first.\nChanllenges Expensive Search  For every instance, it requires O(N) to know its neighbors when a brute force search is used (k-d tree will be faster). How to alleviate it: Approximate Neatest Neighbors  Curse of Dimensionality (CODA) When using Lp-¬≠norm distance measures, averaged distances increase as the dimensionality increases. In a high-dimensional space, most parts are far from all other points.\n As the dimension increases, the ratio of the difference between max and min to the min distance to the center tends to 0. This means all points are equally far apart, leaving no nearest neighbors to learn from.  Advantages/Disadvantages Pros:\n Easy to interpret output Naturally handles multi-class cases Do not need to train Predictive power, can do well in practice with enough representative data  Cons:\n Large search problem to find nearest neighbors Storage of data Curse of dimensionality Must know we have a meaningful distance function  Extension: Approximate Neatest Neighbors Partition: Instead of searching through all points, we will iteratively partition the X space randomly to create subsapces, and search only within those subspaces. We continue partition until we‚Äôve reached designate stopping criteria.  Problem of Partition: You are limited to just the neighbors in the terminal node of the tree. Solution: You don\u0026rsquo;t have to use a single terminal node. Nearby splits (regions) can also be considered.     Clustering Overview  Clustering Clustering is a set of techniques used to partition data into groups, or clusters. Clusters are loosely defined as groups of data objects that are more similar to other objects in their cluster than they are to data objects in other clusters. In practice, clustering helps identify two qualities of data:\n Meaningfulness: Meaningful clusters expand domain knowledge. Usefulness: Useful clusters, on the other hand, serve as an intermediate step in a data pipeline.  Types of Clustering Partitional Clustering  Partitional clustering divides data objects into non-overlapping groups. These techniques require the user to specify the number of clusters, indicated by the variable k. Many partitional clustering algorithms work through an iterative process to assign subsets of data points into k clusters. Two examples of partitional clustering algorithms are k-means and k-medoids. These algorithms are both non-deterministic, meaning they could produce different results from two separate runs even if the runs were based on the same input.  Advantages/Disadvantages Pros:\n They work well when clusters have a spherical shape. They‚Äôre scalable with respect to algorithm complexity.  Cons:\n They‚Äôre not well suited for clusters with complex shapes and different sizes. They break down when used with clusters of different densities.  Density-Based Clustering  Density-based clustering determines cluster assignments based on the density of data points in a region. Clusters are assigned where there are high densities of data points separated by low-density regions. Unlike the other clustering categories, this approach doesn‚Äôt require the user to specify the number of clusters. Instead, there is a distance-based parameter that acts as a tunable threshold. This threshold determines how close points must be to be considered a cluster member. Examples of density-based clustering algorithms include Density-Based Spatial Clustering of Applications with Noise (DBSCAN), and Ordering Points To Identify the Clustering Structure ( OPTICS).  Advantages/Disadvantages Pros:\n They excel at identifying clusters of nonspherical shapes. They‚Äôre resistant to outliers.  Cons:\n They aren‚Äôt well suited for clustering in high-dimensional spaces. They have trouble identifying clusters of varying densities.  Hierarchical Clustering  Hierarchical clustering determines cluster assignments by building a hierarchy. This is implemented by either a bottom-up or a top-down approach. These methods produce a tree-based hierarchy of points called a dendrogram.  Agglomerative clustering is the bottom-up approach. It merges the two points that are the most similar until all points have been merged into a single cluster. Divisive clustering is the top-down approach. It starts with all points as one cluster and splits the least similar clusters at each step until only single data points remain.   Similar to partitional clustering, in hierarchical clustering the number of clusters (k) is often predetermined by the user. Clusters are assigned by cutting the dendrogram at a specified depth that results in k groups of smaller dendrograms. Unlike many partitional clustering techniques, hierarchical clustering is a deterministic process, meaning cluster assignments won‚Äôt change when you run an algorithm twice on the same input data.  Advantages/Disadvantages Pros:\n They often reveal the finer details about the relationships between data objects. They provide an interpretable dendrogram.  Cons:\n They‚Äôre computationally expensive with respect to algorithm complexity. They‚Äôre sensitive to noise and outliers.  K-Means \u0026amp; GMM K-Means Pseudocode    Discussion of K-Means (similiar to KNN)  Distance metrics matters. Most common distance metric is Euclidean distance. Scalling of features matters. Features need to be numeric. Curse of dimensionality.  Evaluation Goodness of fit: determine k Inertia     Inertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:  Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes. Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called ‚Äúcurse of dimensionality‚Äù). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations.    Eblow Method  As k increases, the sum of squared distance tends to zero. Imagine we set k to its maximum value n (where n is number of samples) each sample will form its own cluster meaning sum of squared distances equals zero. If the line chart resembles an arm, then the ‚Äúelbow‚Äù (the point of inflection on the curve) is a good indication that the underlying model fits best at that point.  Silhouette Value  The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from ‚àí1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. The silhouette can be calculated with any distance metric, such as the Euclidean distance or the Manhattan distance.     Cluster Distribution  Do the clusters have practical distribution across them  Interpretation  Do the clusters have meaningful and useful interpretations  Extension: EM        Extension: GMM      ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"e2a30045fa346f6f1eb1a3249ef51baf","permalink":"https://hedygithub.github.io/Di_Portfolio/notes/machine-learning/machine-learning-models/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/Di_Portfolio/notes/machine-learning/machine-learning-models/","section":"notes","summary":"Include: Linear Regression, LASSO and Ridge, Logistic Regression, Naive Bayes, Support Vector Machine, Decision Tree, Ensemble Overview \u0026amp; Stacking, Bagging \u0026amp; Random Forest, Boosting, AdaBoost \u0026amp; Gradient Boosting, K-Nearest Neighbors, Clustering Overview \u0026amp; K-Means\n","tags":null,"title":"Introduction to ML Models","type":"book"},{"authors":null,"categories":null,"content":"Introduction to Machine Learning General knowledge\n  2 hours to read\nMain References  Introduction to Data Science (NYU CDS 1001) Book: Data Science for Business Notes of Probability and Statistics for Data Science (NYU CDS 1002) Optimization and Computational Linear Algebra for Data Science (NYU CDS 1002) Bruce Yang: The Breadth of Machine Learning: Part I K-Means Clustering in Python: A Practical Guide  Data Science Overview  Categories Basic Categories  Supervised Learning  Regression Classfication \u0026amp; Class Probability Estimation   Unsupervised Learning  Clustering    Categories by Parameters  Parametric Modeling  Linear Regression SVM   Non-Parametric Modeling  Decision Tree KNN K-Means    Categories for Classfication  Discrimative  Logistic Regression   Generative  Naive Bayes    Categories by Application  Similarity matching attempts Co-occurrence grouping Profiling (behavior description) Link prediction Data reduction Causal modeling  Cross Industry Standard Process for Data Mining  Business Understaning Data Understanding Data Preparation Modeling Evaluation Deployment  Components: Things Data Scientist has to Consider  Data \u0026amp; Sampling  Define Instance Sampling Cleaning data   Features Representation  Feature Engineering Feature Selection ( \u0026lt;- 4 / 6)   Model ( \u0026lt;- 6) Objective Function / Loss Function (For Model Training) Algorithm for Optimizing (Accelerate the Process) Evaluation Metirc (For Application)  Concerns Concept Drift: P(X), P(Y) or P(Y|X) that changes¬†over time Methods to handle it:\n Monitor predictive performance Retrain as often as possible Test balance between data recency and data volume  Data \u0026amp; Sampling  Think about:  Where to get data How to get data What does the data look like What\u0026rsquo;s the limits  Souces:  Internal ETL Process Production Logging / Sampling Web Scraping / API Survey / Panel  Define the Instance of the Data   What should be sampled?\n Both postitives and negatitives are drawn from the same population or process Only observe positives and find appropriate negatitives    The granularity and range of the instance\n Time Geo Product Level    Are intances independent of each other?\n Geo-Spatial data Time series data Pairwise instances(social networks, search)    Define the Target Variable: Based on Application and Be Creative Sampling  Down-Sampling  Reduce comupational burden of training Require Less Data: Less complex alogrithms \u0026amp; models with information rich features [Check Learning Curves to see the ] Measure empirically the effect of down-sampling (samling size): Learning Curves with X-axis Sample Size.   Up-Sampling / Down-Sampling: Rebalance classes  When do model evaluation, should still based on the real base rate       Selction Bias Implications:  Affect generalizability Affect identifiablity of model parameters  Unbias(random) Sample Test  Independent on X (don\u0026rsquo;t want to be biased): P(Sampled) = P(Sampled | X = x) or P(X = x) = P(X = x | Sampled) Independent on Y (sometimes intentional bias on Y): P(Sampled) = P(Sampled | Y = y) or P(Y = y) = P(Y = y | Sampled)  Intentional Selection Bias  Often select based on target variable It is rational and based on business and economic factors  What to do  Avoid it Adjust it Expect it  Data Cleaning \u0026amp; Exploratory Data Analysis  Goals to do EDA  Summarize main characteristics of the data [Univariate] Gain better understanding of the data set [Univariate] Uncover relationships between variables [Bivariate] Know the data set from a global view [Multivariate] Extract important variables  Descriptive Statistics  Know data types  Numeric  Continuous Discrete   Categorical  Ordinal Nominative   Date   Summariz statistics using pd.describe() Distribution: Box Plots, Scatterplot  Data Cleaning   Missing Values\n Check with data collection source Delete: Random \u0026amp; Rare Fill Constants: Mean, Median, Dummy Variables Exploit Mulit-Collinearity: Estimate E[missing given X]    Data Formating\n Correct data types Apply calculations to incoherent representations    Outliers\n Delete    Scale Difference\n Normalization  Simple Feature Scaling (X / X_max) Min-Max Z-score      Skewed Distribution (for Linear Regression)\n Standardize: Log()    Turning categorical variables into quantitative variables\n One-hot encoding    Bivariate   Correlation for Numerical\n  Covariance Matrix \u0026amp; Heatmap\n  Pros:\n Expresses negative dependencies Well understood and intuitive (easy to communicate)    Cons:\n Can not capture non-linear dependencies better Not apply to categorical data      Mutual Information     Pros:\n Can capture non-linear dependencies better Works naturally with categorical data    ConsÔºö\n Can not express negativedependencies      Numerical variable group by Categorical variable\n Analysis of Variance (ANOVA):  ANOVA: finding correlation between different groups of categorical values F-test: variation between sample group means divided by variation within sample group   Two sample T-test    AUC for Numerical and Categorical\n  Multivariate Singular Value Decomposition The relative difference between singular values is a function of the level of independence of the columns. Applications:\n The low rank approximation \u0026amp; Data Compression  Cost: the information retained ratio   Dimensionality Reduction Recommender Systems Clustering in High Dimensions  Feature Engineering  Note: Deep Learning could do \u0026ldquo;implicit feature engineering, while not all problems will be a Deep Learning problem. Hence we need \u0026ldquo;explicit feature engineering\u0026rdquo;\n  Data Binning\n Group a set of numerical / categorical values into a set of \u0026ldquo;Bins\u0026rdquo;, based on pre-defined values. Could use Clustering ahead to help determine the groups and bins boundaries.    Non-Linear Transformations (Ploynomial Expanion)\n Higher Degree Interaction Terms  The complexity is high, making it infeasible to build and test them all:  A good technique is to run on a Decision Tree ans make interations from the fisrt few split variables. Another method is to use feature importance and make interactions from the more important ones.        Put them into Applications  Noisy and Less Info:  Non-Linear Transformation  Low Degree: Underfitting Higher Degree: Overfitting      Bins  More Bins: hard to borrow information from neighbor bins (using avg Y instead)        Info Rich Environment  Non-Linear Transformation  Higher Degree less likely to be overfit.   Bins:  More bins perform best, because it can approximate any arbitrary curve, but still likely to be overfitting.      Note: some algorithms could do this for you:  SVMs with Kernel: the use of kernel could map the data into an infinite space, no explicit feature engineering needed. Trees: Tree based alogrithms can fit non-linear curves and interactions naturally by partitioning on X and estimating expectations separately for each partition (similiar to binning)  Extract Extra Features  Datetime  Weekday, Weekend Holiday \u0026hellip;   Numerical Variable to Categorical  Binning Clustering    Leakage  Target Variable Leakage: Having features that are caused by the outcome of the target variable Training/Testing Leakage:  Having records in the training set also appear in the test set. [Time Series Related ]Training has features that can not get at the time before testing.    Loss Function \u0026amp; Evaluation Metrics  Loss Function For Classification Ref. Loss_functions_for_classification    0-1 Loss (not convex) Surrogate Loss (convex)  Logistic Loss / Cross Entropy Loss / Log-Likelihood (LL): Logistic Regression Hinge Loss: SVM Exponential Loss: Boosting Note: The validation loss metric does not have to be the same as the training loss. Sometimes the loss metric for an application (i.e., AUC for validation) is not easy to directly minimize. Insteat we use other metric in training (i.e., logistic loss instead)    Loss Function for Regression  Mean Squared Error (MSE) Mean Absolute Loss  Evaluation Metrics Confusion Matrix   Accuracy: Most intuitive and well known\n Base Rate Dependent    Precision: Of all the instances which the model predict as positive, how many are real positive?\n Best used when False Positives are relatively expensive, i.e. budget is limited Base Rate Dependent    Recall: Of the totoal real positives, how many did the model predict as positive?\n Best used when False Negatives are relatively more expensive, i.e. test tumor Base Rate Independent       F1-Score: Favor both precision and recall. It is the harmonic mean of the two.     Lift: How many more positives outcomes you might expect relative to the baseline stratgey (random guess).\n Used with Recall to do economic analysis (help to decide the threshold)    ROC Curve, AUC, ACLC   ROC Curve: the Receiver Operating Characteristic curve.\n True Positive Rate: True Positive / Golden Negative False Negative Rate: False Negative / Golden Positive    AUC: Area Under the ROC Curve\n Probability Interpretation: The AUC is the probability the model will score a randomly chosen positive class higher than a randomly chosen negative class. Invariance to prior class probabilities or class prevalence in the data. Useful for comparing across data sets with different base rates or after down sampling. Independence of the decision threshold. Is nicely bounded [0, 1]       ACLC: Area under the Cumulative Lift Curve\n  Expected Value and Cost Curve  Expected Value: Help to choose/change a decision threshold based on cost-benefit analysis    Cost Curve : Used in unequal cost scenario. The area measures the expected total costs.     Metrics for Different Applications  Ranking (Without Threshold)  AUC, AULC   Classification  Accuracy, Precision, Recall, F-Score, Lift   Density Estimaion (Numerical)  Regression: MSE, MAE Classification Related: Surrogate Losses    Model Selection  Rules:  Using the same training and validation data for each hypothesis being tested Given a tie (statistical or exact), choose the simpler model, i.e. first std error rule Use this methodology for all design decisions:  Hyper-Parameter Selection Feature Selection Model Selection    Hyper-Parameter Selection Hyper-Parameter Examples  Linear Regression \u0026amp; Logistic Regression  L1 / L2: regularization strategy C: regularization weight   Support Vector Machine  C: regularization weight Kernel and its associated hyperparameters   Decision Tree  MaxDepth, Min LeafSize, MinSplitSize   Random Forest  Tree Related Forest Related    Method   Training-Validation-Test\n Training: the training data is used to find the optimal function given the model structure (i.e., fixed algorithm, feature set) Validation: the validation data is used to evaluate the loss/risk for a given model configuration. The configuration with the besr loss/risk is selected as the final model Test: test data is not used for any parameter or model selection. It is only used as a generalization measure.      Note: Training error is our empirical risk and the test set error is our approximation of expected risk. Note (for training part): Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true \u0026ldquo;risk\u0026rdquo;) because we don\u0026rsquo;t know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the \u0026ldquo;empirical\u0026rdquo; risk). Note (for validation part): Rules to Choose Hyper-Parameter in Validation Set:  Max / Min (validation loss metric) One-StdError Rule: The one first hits: Max / Min (validation loss metric) - One-StdError      Cross Validation \u0026ldquo;Recycle\u0026rdquo; data using k-fold cross validation as validation scheme.    Apply to: SVM, DT\u0026hellip; Note: Random Forest use out-of-bag error rather than error of cross-validation set (RF Based on Bootstraping) Note: Training error is our empirical risk and the test set error is our approximation of expected risk.    Nested Cross Validation\n    Apply to: Time Series Data Note: How to split?       How to Choose Candidate for Hyper-Parameters  Range \u0026amp; Numers in this Range  Should span the range of low to high model complexity   Method:  Grid Search Random Search    Feature Selection Why Perform Feature Selection?  Lower expected model variance (less likely to be overfitting) Easier interpretation of models Better scalability, both in training and deployment Lower maintenance costs  Common Feature Selection Techniques  Naive Subset Selection  Pre-filter features based on heuristics Choose top k based on:  Mutual information, Correlation with Y Has the most coverage / Support (non-na, non-zero percentage)   Application: bag-of-words selections (long-tail)   Best Subset Selection  Choose the best subst of k features from p features High complexity   Stepwise Selection  Incrementally add/subtract features until model performance stabilizes Greedy: incrementally select the kth feature which improve the performance most k could also be seen as hyper-parameters      Dimensionality reduction  take rank-k approximations of X using SVD   Regularization  Implicit, based on adding complexity penalties to loss function    Alogrithm Selection Types of Alogrithms  Classic \u0026amp; Simplier Methods:  Linear Regression Decision Tree Naive Bayes K-Nearest Neighbors   Black Box but Powerful Methods  Random Forests SVM with Kernel Neural Networks    Methods for Model Selection  First, consider all constraints of the problem, and choose alogrithms under constraints.  Too little data (generally an estimation problem) Too much data (generally a computation problem) The assumptions of candidate alogrithms Easy to interpret the model? Does scalability matter? (training time, predicting time, model storage)   Try all of them, choose best performer based on evaluation metric  Be Agile: iterate  Start with a resonable baseline model: the one with little effort but sophisticated enough to capture signals. Iterate towards better models: measure cost-revenue at every iteration    ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"95a522624536ab9e1fbb62ac2aa63e78","permalink":"https://hedygithub.github.io/Di_Portfolio/notes/machine-learning/machine-learning-general/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/Di_Portfolio/notes/machine-learning/machine-learning-general/","section":"notes","summary":"Introduction to Machine Learning General knowledge\n","tags":null,"title":"ML General Knowledge","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://hedygithub.github.io/Di_Portfolio/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/Di_Portfolio/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://hedygithub.github.io/Di_Portfolio/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/Di_Portfolio/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://hedygithub.github.io/Di_Portfolio/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/Di_Portfolio/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://hedygithub.github.io/Di_Portfolio/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/Di_Portfolio/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"","date":1620604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620604800,"objectID":"a2452fb60fe8334e434fb2611b43dd5f","permalink":"https://hedygithub.github.io/Di_Portfolio/notes/deep-learning/","publishdate":"2021-05-10T00:00:00Z","relpermalink":"/Di_Portfolio/notes/deep-learning/","section":"notes","summary":"This course concerns the latest techniques in deep learning and representation learning, focusing on supervised and unsupervised deep learning, embedding methods, metric learning, convolutional and recurrent nets, with applications to computer vision, natural language understanding, and speech recognition.","tags":null,"title":"Deep Learning","type":"notes"},{"authors":null,"categories":null,"content":"Introduction At the end of Deep Learning course taught by Yann LeCun, a final project was assigned. We competed with your classmates on who can find the best self/semi-supervised learning algorithm. We were given a dataset with a large amount of unlabeled data and a small amount of labeled data to train the model, and the final performance of the model will be evaluated on a hidden test set and posted on a public leaderboard.\nData Overview The dataset, of color images of size 96√ó96, that has the following structure\n 512, 000 unlabeled images. 25, 600 labeled training images (32 examples, 800 classes). 25, 600 labeled validation images (32 examples, 800 classes).  Overall, a hidden test set was kept, which will be used to test the models. In order to improve performance when training our model with few labeled samples, we need to make use of the unlabeled data.\nOur presentation Semi-supervised learning incorporates both labeled and unlabeled data points in the training process and aims to use leverage the unlabeled data to learn features that would support modeling process when using the training data for specific tasks. The community has gained popularity as the amount of data required and cost of obtaining human labeled data increased over the years. This video explains the modeling and thought process our team had when tackling the given task, and achieved 50% accuracy on the test set with CoMatch.\n  All presentations at Deep Learning Spring 21 Class   ","date":1620604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620604800,"objectID":"e372e9acff694f2c88872b21abe5c7bc","permalink":"https://hedygithub.github.io/Di_Portfolio/project/ssl-image-classfication/","publishdate":"2021-05-10T00:00:00Z","relpermalink":"/Di_Portfolio/project/ssl-image-classfication/","section":"project","summary":"Competed on finding the best Self/Semi-Supervised Learning (SSL) algorithm for a 512,000-images classification task with only 5% labelled from 800 classes. Researched and trained contrastive learning models (SimCLR, SimSiam, BarlowTwins), and pseudo labeling models (FixMatch, CoMatch) via PyTorch on Greene Cluster. Achieved 51% accuracy on the testing set and got 2nd place.","tags":["Deep Learning"],"title":"Self/Semi-Supervised Image Classification","type":"project"},{"authors":null,"categories":null,"content":"Introduction In this final project, we apply big data tools to build and evaluate a collaborative filter based recommender system. The dataset we work on is the Million Song Dataset (MSD), with implicit user feedback. Using Spark‚Äôs alternating least squares (ALS) method, we learn latent factor representations for users and items, and recommend for users in the test set. Thereafter, we compare our model to single-machine implementation, and the baseline for the extension.\nData Overview Data used for the basic recommendation system consists of the train, validation, and test parquet files. Each row in the files consists of user_id (string), count (int), and track_id (string). There are 49,824,519 records for the train, 135,938 records for the validation, and 1,368,430 for the test. Additional data including metadata, features, genre tags, lyric, are also used for the extension\nReport Please review the report pdf for details.\n","date":1619740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619740800,"objectID":"435b59e749f5dfcc50e91299d2724694","permalink":"https://hedygithub.github.io/Di_Portfolio/project/recommender-system/","publishdate":"2021-04-30T00:00:00Z","relpermalink":"/Di_Portfolio/project/recommender-system/","section":"project","summary":"Trained alternating least squares (ALS) model via PySpark on the Million Song Dataset to get latent representation for users and songs. Used PCA to reduce dimension and visualize music genres.","tags":["Machine Learning"],"title":"Pyspark Song Recommender System","type":"project"},{"authors":null,"categories":null,"content":"Introduction Sentiment Analysis (SA) aims at classifying the sentiment polarity towards a whole sentence. Compared to SA, Aspect-Based Sentiment Analysis (ABSA) is designed to identify certain target aspects of an entity and classify sentiment polarities towards these aspects. For example, in the sentence ‚ÄúFood is pretty good but the service is horrific‚Äù, there are two target aspects: ‚Äúfood‚Äù and ‚Äúservice‚Äù with opposite sentiment polarities. Further, there are two variants of the ABSA problem. One is Aspect-Target Sentiment Classification (ATSC), which is our example before and also the focus of our paper.\nIn recent years, neutral networks have been developed and largely improved the ABSA performance by learning target-context relationships. Afterwards, the pre-trained language model shows powerful representation ability. Its application to many down-stream tasks, including ABSA, has achieved many accomplishments. Lately, domainspecific post-trained BERT shows better performance on this topic. In this paper, we experiment with LSTM-based and BERT-based models for aspect-based sentiment analysis, and apply these models to a more challenging dataset than the commonly used benchmark. We then conduct a robust error analysis for our models to analyze reasons for erroneous classifications.\nOur presentation We implement LSTM-based models (LSTM and ATAE-LSTM) and BERT-based models (vanilla BERT-base and BERT-ADA) on a challenging dataset called MAMS, which contains multiple aspects with multiple sentiment polarities. We also conduct error analysis through the method of input reduction. The experiment results show that BERT-ADA outperforms other models on MAMS dataset. You can find more details in the video.\n  ","date":1607558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607558400,"objectID":"9a72fe07489272d55ad434e31dd8abfe","permalink":"https://hedygithub.github.io/Di_Portfolio/project/nlp-absa/","publishdate":"2020-12-10T00:00:00Z","relpermalink":"/Di_Portfolio/project/nlp-absa/","section":"project","summary":"Predicted aspect-level sentiment polarities on a restaurant reviews dataset (MAMS) that contains multiple aspects with multiple sentiment polarities. Implemented LSTM model and fine-tuned BERT based model. Achieved 84% accuracy on MAMS and performed error analysis.","tags":["NLP"],"title":"Aspect-Based Sentiment Analysis","type":"project"},{"authors":null,"categories":null,"content":"","date":1607558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607558400,"objectID":"6e5a236483902322950878d995eab920","permalink":"https://hedygithub.github.io/Di_Portfolio/project/bilibili/","publishdate":"2020-12-10T00:00:00Z","relpermalink":"/Di_Portfolio/project/bilibili/","section":"project","summary":"Bilibili is the most popular creator-driven content video play platform in China. To help vloggers better understand drivers of popular videos, we predicted the number of views for new videos based on video titles and labels, vlogger‚Äôs and previous video‚Äôs information using models include Ridge, Random Forest, XGBoost. We found features like video length, video posting frequency and time also play an important role in video popularity except previous video plays.","tags":["Machine Learning"],"title":"Predict number of views for videos on Bilibili","type":"project"},{"authors":null,"categories":null,"content":"Introduction This project was originally created by former TA Alan Yao (now at AirBNB). It is a web mapping application inspired by Alan‚Äôs time on the Google Maps team and the OpenStreetMap project, from which the tile images and map feature data was downloaded.\nIn this project, you‚Äôll build the ‚Äúsmart‚Äù pieces of a web-browser based Google Maps clone. This is typical in real world programming, where you don‚Äôt have the luxury and freedom that comes with starting from totally blank files. As a result, you‚Äôll need to spend some time getting to know the provided code, so that you can complete the parts that need completing. The map should have these functions:\n Zoom in and zoom out the map. Autocomplete the search input. Prefix is the partial query string. The prefix will be a cleaned name for search that is: (1) everything except characters A through Z and spaces removed, and (2) everything is lowercased. The method will return a list containing the full names of all locations whose cleaned names share the cleaned query string prefix, without duplicates. Search. The user should also be able to search for places of interest. Implementing this method correctly will allow the web application to draw red dot markers on each of the matching locations. Turn-by-turn navigation. As an extra-challanging feature, you can use your A* search route to generate a sequence of navigation instructions that the server will then be able to display when you create a route.  By the end of this project, with some extra work, you can even host your application as a publicly available web app. More details will be provided at the end of this spec.\nMore details about the project can be found: https://sp19.datastructur.es/materials/proj/proj2c/proj2c#introduction\nMy Publicly Available Web App on Heroku http://bearmaps-cs61b-sp19-hedy.herokuapp.com/map.html#lat=37.871826\u0026lon=-122.260086\u0026depth=3\n","date":1596067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596067200,"objectID":"7015dbec5157892ef26f4c3254da997f","permalink":"https://hedygithub.github.io/Di_Portfolio/project/map/","publishdate":"2020-07-30T00:00:00Z","relpermalink":"/Di_Portfolio/project/map/","section":"project","summary":"With tile images and map feature data was downloaded, build a web mapping application using java. The data structures \u0026 alogrithms maily used are Graph, A*.","tags":["Other"],"title":"Bear Maps","type":"project"},{"authors":null,"categories":null,"content":"Introduction  With the fast development of China consumer market, a global fast-fashion brand plans to enter China market. And they have chosen Shanghai, the most fashion city in China, as their first entered city. At first stage, the company plans to open about 8~10 stores, based on their judgments of the market and company‚Äôs financial ability. The business problem is where are the best locations to open those stores. Without considering rent costs, the company should open a store in the most popular place, where bring stores high traffic and target customers. Therefore, we must know where the company‚Äôs target customer mostly like to visit. It can be breakdown to two questions. First, how to find the place our target customers like to visit. In this case, our customer is those 18~30 girls who like fashion clothes, shoes, and bags. Second, where is the most popular places among all the places they like. Another problem is to decide the store opening sequence and the possible shopping malls near those optimal locations to open those stores. Maybe we can rank those popular places according to their popularity and open store according to this rank. And we can find nearby shopping malls as a list to give the company‚Äôs mangers as a reference.  Report Suggested new store locations using K-means and DBSCAN clustering with parsed geolocation data including public transport locations, E-commerce delivery addresses, competitors' store locations. More details in the pdf and slides.\n","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585526400,"objectID":"39d24b948752e730ec62ae9bbc45f994","permalink":"https://hedygithub.github.io/Di_Portfolio/project/store-location/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/Di_Portfolio/project/store-location/","section":"project","summary":"Suggested new store locations using K-means and DBSCAN clustering with parsed geolocation data including public transport locations, E-commerce delivery addresses, competitors' store locations.","tags":["Machine Learning"],"title":"Suggested Store Locations","type":"project"},{"authors":["Di He"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://hedygithub.github.io/Di_Portfolio/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/Di_Portfolio/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Di He"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')     print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://hedygithub.github.io/Di_Portfolio/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/Di_Portfolio/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://hedygithub.github.io/Di_Portfolio/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/Di_Portfolio/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Di He","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://hedygithub.github.io/Di_Portfolio/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/Di_Portfolio/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Di He","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://hedygithub.github.io/Di_Portfolio/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/Di_Portfolio/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://hedygithub.github.io/Di_Portfolio/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/Di_Portfolio/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://hedygithub.github.io/Di_Portfolio/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/Di_Portfolio/projects/","section":"","summary":"","tags":null,"title":"Projects","type":"widget_page"}]